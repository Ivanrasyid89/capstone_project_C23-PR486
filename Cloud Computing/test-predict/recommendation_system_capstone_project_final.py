# -*- coding: utf-8 -*-
"""Recommendation_System_Capstone_Project_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-o6yqfMc6oGeI_apdk7l34Urm1AsoOdn

# Import Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

#from google.colab import drive
#drive.mount('/content/drive')

"""# Import Files"""

pd.set_option("display.precision", 15)
data_user = pd.read_csv('data_user_gabungan.csv')
data_shelter = pd.read_excel('shelter.xlsx')

data_user.head()

data_shelter.head()

"""# Modifying Dataframes"""

# Create function to seperate latitude and longitude values
def latitude_value(row):
  return row.replace(' ','').split(',')[0]

def longitude_value(row):
  return row.replace(' ','').split(',')[1]

# Apply the function to seperate latitude and longitude values
data_user['longitude'] = data_user['latitude'].apply(lambda row: longitude_value(row))
data_user['latitude'] = data_user['latitude'].apply(lambda row: latitude_value(row))

# Restructuring the order of columns
data_user = data_user[['alamat_user','latitude','longitude','jenis_hewan','nama_shelter']]

data_shelter = data_shelter.drop(columns=['Unnamed: 5'])
data_shelter = data_shelter.dropna()

data_shelter = data_shelter[['alamat_shelter','latitude','longitude','jenis_hewan','nama_shelter']]

# Convert 'latitude' and 'longitude' column into float type
data_user = data_user.astype({'latitude': np.float32, 'longitude': np.float32})

# Convert 'nama_shelter' into numerical

## Define/Initialize the Label Encoder
le_user = LabelEncoder()

## Fit the Label Encoder and use it to transform the target ('nama_shelter' column)
data_user['nama_shelter'] = le_user.fit_transform(data_user['nama_shelter'])

data_user.head()

data_shelter.head()

data_user_2 = data_user.copy()

data_user_2 = data_user_2[['latitude','longitude','jenis_hewan','nama_shelter']]

data_user_2

"""# Split the Dataset into Train and Test"""

# Split the data into train and test
train, test = train_test_split(data_user_2, test_size=0.1, random_state=0)

# Split train data into train and validation data
train, validation = train_test_split(train, test_size=0.1, random_state=0)

print('Number of data for training:', len(train), 'rows')
print('Number of data for validating:', len(validation), 'rows')
print('Number of data for testing:', len(test), 'rows')

"""# Preparing the Dataset for input to the Deep Learning Model"""

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('nama_shelter')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))

  if shuffle == True:
    ds = ds.shuffle(buffer_size=len(dataframe))

  ds = ds.batch(batch_size=batch_size)

  return ds

# Define the batch_size to be used
batch_size = 64

# Converting the train and test datframes into tfds dataset
train_ds = df_to_dataset(train, batch_size=batch_size)
validation_ds = df_to_dataset(validation, shuffle=False)
test_ds = df_to_dataset(test, shuffle=False)

output_num = data_user_2['nama_shelter'].nunique()
print(output_num)

"""## Take a Look at Input Pipeline"""

# Take a look at a single batch
for feature, label in train_ds.take(1):
  print('Every feature:', list(feature.keys()))
  print('A batch of ages:', feature['jenis_hewan'])
  print('A batch of targets:', label)

"""# Create Several Types of Feature Columns"""

feature_columns = []

# Numeric columns: 'latitude','longitude'
numeric_columns = ['latitude','longitude']

for col in numeric_columns:
  numeric_feature_column = tf.feature_column.numeric_column(col)
  feature_columns.append(numeric_feature_column)

# Categorical columns: 'jenis_hewan'
jenis_hewan = tf.feature_column.categorical_column_with_vocabulary_list('jenis_hewan',['Kucing','Anjing','Lainnya'])
jenis_hewan_one_hot = tf.feature_column.indicator_column(jenis_hewan)
feature_columns.append(jenis_hewan_one_hot)

"""## Create a Feature Layer"""

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

"""# Model"""

K.clear_session()

model = tf.keras.Sequential([
    feature_layer,
    tf.keras.layers.Dense(units=128, activation='relu'),
    tf.keras.layers.Dense(units=128, activation='relu'),
    tf.keras.layers.Dense(units=256, activation='relu'),
    tf.keras.layers.Dense(units=output_num, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_ds, validation_data=validation_ds, epochs=1000)

model.evaluate(test_ds)

for key, value in test_ds.take(1):
  print(key, value)

"""# Test Input Pipeline from User"""

-8.783392133495186, 115.17229372758489

# Create input API
latitude_input = float(input('Masukkan nilai latitude: '))
longitude_input = float(input('Masukkan nilai longitude: '))
jenis_hewan_input = str(input('Masukkan jenis hewan yang diminati: '))

# Create dataframe to contain user's inputs
df_for_test_predict = pd.DataFrame(data={'latitude':[latitude_input], 
                                         'longitude':[longitude_input], 
                                         'jenis_hewan':[jenis_hewan_input]})

df_for_test_predict

# Create function to convert user's inputs dataframe into tensorflow ds
def df_user_input_to_ds(df_user):
  df_user = df_user.copy()
  ds = tf.data.Dataset.from_tensor_slices(dict(df_user))
  ds = ds.batch(1)
  return ds

predict_ds = df_user_input_to_ds(df_for_test_predict)

# Model prediction
model_prediction = model.predict(predict_ds)

model_prediction

prediction_result = model_prediction[0]

# Get the prediction result from model
prediction_result

# Get the highest index of prediction_index array
highest_prediction_index = np.argmax(prediction_result)

highest_prediction_index

# Give the result back to the user
output_to_user = le_user.inverse_transform([highest_prediction_index])

output_to_user

"""# Exporting to TFLite"""

export_dir = 'saved_model/1/'
tf.saved_model.save(model, export_dir=export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_model = converter.convert();

import pathlib

tflite_model_file = pathlib.Path('saved_model/1//model.tflite')
tflite_model_file.write_bytes(tflite_model)